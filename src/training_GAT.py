# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YH2dkAbBJrx4DMX9cBzfF-WT23fIcJOs

# Tiền xử lý
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Đọc dataset từ file CSV
df = pd.read_csv("traffic_accidents.csv")


# Xóa cột "crash_date" nếu tồn tại
if "crash_date" in df.columns:
    df = df.drop(columns=["crash_date"])

# Các cột cần mã hóa
categorical_columns = [
    "traffic_control_device", "weather_condition", "lighting_condition", "first_crash_type",
    "trafficway_type", "alignment", "roadway_surface_cond", "road_defect", "crash_type",
    "intersection_related_i", "damage", "prim_contributory_cause", "most_severe_injury"
]

# In ra tên các cột trong DataFrame để kiểm tra
print(df.columns)

# Dictionary để lưu các label encoder
decoders = {}

# Mã hóa từng cột
for col in categorical_columns:
    # Kiểm tra xem cột có tồn tại trong DataFrame không
    if col in df.columns:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])  # Thay đổi trực tiếp giá trị trong cột
        decoders[col] = dict(zip(le.classes_, le.transform(le.classes_)))  # Lưu mapping cho báo cáo
    else:
        print(f"Cột '{col}' không tồn tại trong DataFrame.")

# Xuất báo cáo mã hóa
encoding_report = """Báo cáo Mã hóa Categorical Data\n\n"""
for col, mapping in decoders.items():
    encoding_report += f"Cột: {col}\n"
    for key, value in mapping.items():
        encoding_report += f"  {key}: {value}\n"
    encoding_report += "\n"

# Xuất báo cáo mã hoá ra file
with open("encoding_report.txt", "w", encoding="utf-8") as f:
    f.write(encoding_report)

# Lưu dataset đã mã hóa
encoded_file = "encoded_dataset.csv"
df.to_csv(encoded_file, index=False)

print(f"Mã hóa hoàn tất! Dataset đã lưu vào: {encoded_file}")
print(f"Báo cáo mã hóa đã lưu vào: encoding_report.txt")

# Thông tin tổng quan về dataset sau xử lý
print("Dataset Shape:", df.shape)

df.info()

df.isnull().sum()

df.describe()

# Kiểm tra phân phối dữ liệu của các cột đã mã hóa
print("\nPhân phối dữ liệu của một số cột đã mã hóa:")
for col in categorical_columns[:5]:
    print(f"\nPhân phối của cột {col}:")
    print(df[col].value_counts())

df

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from itertools import combinations

# Đọc dataset
file_path = 'encoded_dataset.csv'  # Đặt đường dẫn dataset của bạn
df = pd.read_csv(file_path)

df = df.sample(2000) # Lấy ngẫu nhiên 2000 dòng dữ liệu

# Khởi tạo đồ thị
G = nx.Graph()

# Thêm nút cho mỗi vụ tai nạn (dựa trên index)
for index, row in df.iterrows():
    G.add_node(index,
               traffic_control_device=row['traffic_control_device'],
               weather_condition=row['weather_condition'],
               lighting_condition=row['lighting_condition'],
               first_crash_type=row['first_crash_type'],
               trafficway_type=row['trafficway_type'],
               alignment=row['alignment'],
               roadway_surface_cond=row['roadway_surface_cond'],
               road_defect=row['road_defect'],
               crash_type=row['crash_type'],
               intersection_related_i=row['intersection_related_i'],
               damage=row['damage'],
               prim_contributory_cause=row['prim_contributory_cause'],
               num_units=row['num_units'],
               most_severe_injury=row['most_severe_injury'],
               injuries_total=row['injuries_total'],
               injuries_fatal=row['injuries_fatal'],
               injuries_incapacitating=row['injuries_incapacitating'],
               injuries_non_incapacitating=row['injuries_non_incapacitating'],
               injuries_reported_not_evident=row['injuries_reported_not_evident'],
               injuries_no_indication=row['injuries_no_indication'],
               crash_hour=row['crash_hour'],
               crash_day_of_week=row['crash_day_of_week'],
               crash_month=row['crash_month'])

# Hàm kiểm tra điều kiện kết nối giữa hai vụ tai nạn
def is_similar(accident1, accident2):
    # Các điều kiện tương tự dựa trên các đặc trưng quan trọng
    time_diff = abs(accident1['crash_hour'] - accident2['crash_hour']) <= 1
    same_month = accident1['crash_month'] == accident2['crash_month']
    same_day_of_week = accident1['crash_day_of_week'] == accident2['crash_day_of_week']
    same_trafficway = accident1['trafficway_type'] == accident2['trafficway_type']
    same_crash_type = accident1['first_crash_type'] == accident2['first_crash_type']
    same_injury_no_indication = accident1['injuries_no_indication'] == accident2['injuries_no_indication']

    # Kết nối nếu ít nhất một điều kiện tương tự
    return (time_diff or same_month or same_day_of_week or same_trafficway or
            same_crash_type or same_injury_no_indication)

# Thêm các cạnh dựa trên tính tương đồng
for u, v in combinations(G.nodes(data=True), 2):
    if is_similar(u[1], v[1]):
        G.add_edge(u[0], v[0])

print("Đồ thị G đã được tạo với", G.number_of_nodes(), "nút và", G.number_of_edges(), "cạnh.")

# Đọc toàn bộ file encoding_report.txt
file_path = "encoding_report.txt"

with open(file_path, "r", encoding="utf-8") as f:
    content = f.read()

print("Nội dung của encoding_report.txt:")
print(content)

"""# Huấn luyện"""

pip install torch-geometric

import torch
import torch.nn.functional as F
from torch_geometric.nn import GATConv, LayerNorm
from torch_geometric.data import Data
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE

# Xử lý mất cân bằng dữ liệu bằng over-sampling
class FocalLoss(torch.nn.Module):
    def __init__(self, alpha=None, gamma=2.0):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, logits, targets):
        ce_loss = F.cross_entropy(logits, targets, weight=self.alpha, reduction="none")
        pt = torch.exp(-ce_loss)  # Tính xác suất dự đoán đúng
        focal_loss = (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()

# Chuyển đổi đồ thị sang PyG Data
def networkx_to_pyg(G, label_attr="damage"):
    node_mapping = {node: i for i, node in enumerate(G.nodes())}
    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in G.edges()], dtype=torch.long).t().contiguous()

    features = []
    labels = []
    for node, data in G.nodes(data=True):
        node_features = [data[attr] for attr in data if attr != label_attr]
        features.append(node_features)
        labels.append(data[label_attr])

    X = torch.tensor(features, dtype=torch.float)
    y = torch.tensor(labels, dtype=torch.long)

    return Data(x=X, edge_index=edge_index, y=y)

# Chuyển đổi đồ thị
graph_data = networkx_to_pyg(G)

# Cân bằng dữ liệu bằng SMOTE
X_resampled, y_resampled = SMOTE().fit_resample(graph_data.x.numpy(), graph_data.y.numpy())
graph_data.x = torch.tensor(X_resampled, dtype=torch.float)
graph_data.y = torch.tensor(y_resampled, dtype=torch.long)

# Chia tập train/test/val
train_nodes, remaining_nodes = train_test_split(range(graph_data.num_nodes), test_size=0.4, random_state=42)
test_nodes, val_nodes = train_test_split(remaining_nodes, test_size=0.25, random_state=42)

graph_data.train_mask = torch.tensor(train_nodes, dtype=torch.long)
graph_data.test_mask = torch.tensor(test_nodes, dtype=torch.long)
graph_data.val_mask = torch.tensor(val_nodes, dtype=torch.long)

# Trọng số class cho Focal Loss
unique, counts = np.unique(graph_data.y.numpy(), return_counts=True)
class_weights = torch.tensor(1.0 / counts, dtype=torch.float)
class_weights /= class_weights.sum()
loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)

# Mô hình GAT
class GAT(torch.nn.Module):
    def __init__(self, in_features, hidden_dim, out_features, heads=8):
        super(GAT, self).__init__()
        self.gat1 = GATConv(in_features, hidden_dim, heads=heads, dropout=0.2)
        self.ln1 = LayerNorm(hidden_dim * heads)  # LayerNorm thay cho BatchNorm

        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=4, dropout=0.2)
        self.ln2 = LayerNorm(hidden_dim * 4)

        self.gat3 = GATConv(hidden_dim * 4, out_features, heads=1, concat=False, dropout=0.2)

        self.dropout = torch.nn.Dropout(0.1)

    def forward(self, x, edge_index):
        x = self.gat1(x, edge_index)
        x = self.ln1(x)
        x = F.elu(x)
        x = self.dropout(x)

        x = self.gat2(x, edge_index)
        x = self.ln2(x)
        x = F.elu(x)
        x = self.dropout(x)

        x = self.gat3(x, edge_index)
        return F.log_softmax(x, dim=1)

# Khởi tạo mô hình
model = GAT(in_features=graph_data.x.shape[1], hidden_dim=16, out_features=len(graph_data.y.unique()))
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-4)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)

# Huấn luyện mô hình
epochs = []
accuracies = []
f1_scores = []
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    out = model(graph_data.x, graph_data.edge_index)
    loss = loss_fn(out[graph_data.train_mask], graph_data.y[graph_data.train_mask])
    loss.backward()
    optimizer.step()
    scheduler.step(loss)

    # Đánh giá mô hình
    model.eval()
    with torch.no_grad():
        logits = model(graph_data.x, graph_data.edge_index)
        pred = logits.argmax(dim=1)

        y_true = graph_data.y[graph_data.test_mask].cpu().numpy()
        y_pred = pred[graph_data.test_mask].cpu().numpy()

        acc = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred, average='macro')

        epochs.append(epoch)
        accuracies.append(acc)
        f1_scores.append(f1)

        if epoch % 20 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item():.4f}, Accuracy: {acc:.4f}, F1-score: {f1:.4f}")

# Tính confusion_matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")


# In confusion_matrix dạng văn bản
print("Confusion Matrix:")
print(cm)

plt.figure(figsize=(10, 5))
plt.plot(epochs, accuracies, label='Accuracy', marker='o')
plt.plot(epochs, f1_scores, label='F1-score', marker='x')
plt.xlabel('Epoch')
plt.ylabel('Giá trị')
plt.title('Accuracy và F1-score theo Epoch')
plt.legend()
plt.grid(True)
plt.show()
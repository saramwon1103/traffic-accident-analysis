{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee77590",
   "metadata": {},
   "source": [
    "T·∫°o random 100 d·ªØ li·ªáu m·ªõi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992132a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# S·ªë d√≤ng d·ªØ li·ªáu m·ªõi b·∫°n mu·ªën t·∫°o\n",
    "n_samples = 100\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu m·∫´u\n",
    "df_new = pd.DataFrame({\n",
    "    'traffic_control_device': np.random.choice(['SIGNAL', 'STOP', 'NONE'], n_samples),\n",
    "    'weather_condition': np.random.choice(['CLEAR', 'RAIN', 'SNOW'], n_samples),\n",
    "    'lighting_condition': np.random.choice(['DAYLIGHT', 'DARK'], n_samples),\n",
    "    'first_crash_type': np.random.choice(['REAR END', 'ANGLE', 'SIDESWIPE'], n_samples),\n",
    "    'trafficway_type': np.random.choice(['ONE-WAY', 'TWO-WAY'], n_samples),\n",
    "    'alignment': np.random.choice(['STRAIGHT', 'CURVED'], n_samples),\n",
    "    'roadway_surface_cond': np.random.choice(['DRY', 'WET'], n_samples),\n",
    "    'road_defect': np.random.choice(['NONE', 'HOLE'], n_samples),\n",
    "    'crash_type': np.random.choice(['COLLISION', 'NON-COLLISION'], n_samples),\n",
    "    'intersection_related_i': np.random.choice(['Y', 'N'], n_samples),\n",
    "    'damage': np.random.choice(['OVER $1500', 'UNDER $1500'], n_samples),\n",
    "    'prim_contributory_cause': np.random.choice(['DISTRACTION', 'SPEEDING'], n_samples),\n",
    "    'num_units': np.random.randint(1, 5, n_samples),\n",
    "    'most_severe_injury': np.random.choice(['NO INJURY', 'INCAPACITATING INJURY'], n_samples),\n",
    "    'injuries_total': np.random.randint(0, 4, n_samples),\n",
    "    'injuries_fatal': np.random.randint(0, 1, n_samples),\n",
    "    'injuries_incapacitating': np.random.randint(0, 2, n_samples),\n",
    "    'injuries_non_incapacitating': np.random.randint(0, 2, n_samples),\n",
    "    'injuries_reported_not_evident': np.random.randint(0, 2, n_samples),\n",
    "    'injuries_no_indication': np.random.randint(0, 3, n_samples),\n",
    "    'crash_hour': np.random.randint(0, 24, n_samples),\n",
    "    'crash_day_of_week': np.random.randint(1, 8, n_samples),   # 1‚Äì7\n",
    "    'crash_month': np.random.randint(1, 13, n_samples),        # 1‚Äì12\n",
    "})\n",
    "\n",
    "# L∆∞u file CSV\n",
    "df_new.to_csv('new_data_simulated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920cc748",
   "metadata": {},
   "source": [
    "Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu m·ªõi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a5b4adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ƒê·ªçc dataset t·ª´ file CSV\n",
    "df = pd.read_csv(r\"D:\\NƒÉm 3 - HK2\\M·∫°ng x√£ h·ªôi\\traffic-accident-analysis\\data\\new_data_simulated.csv\", encoding='utf-8')  # Th·ª≠ v·ªõi utf-8 ho·∫∑c ISO-8859-1\n",
    "\n",
    "# X√≥a c·ªôt \"crash_date\" n·∫øu t·ªìn t·∫°i\n",
    "if \"crash_date\" in df.columns:\n",
    "    df = df.drop(columns=[\"crash_date\"])\n",
    "\n",
    "if \"damage\" in df.columns:\n",
    "    df = df.drop(columns=[\"damage\"])\n",
    "\n",
    "# C√°c c·ªôt c·∫ßn m√£ h√≥a\n",
    "categorical_columns = [\n",
    "    \"traffic_control_device\", \"weather_condition\", \"lighting_condition\", \"first_crash_type\", \n",
    "    \"trafficway_type\", \"alignment\", \"roadway_surface_cond\", \"road_defect\", \"crash_type\", \n",
    "    \"intersection_related_i\", \"prim_contributory_cause\", \"most_severe_injury\"\n",
    "]\n",
    "\n",
    "# Dictionary ƒë·ªÉ l∆∞u c√°c label encoder\n",
    "decoders = {}\n",
    "\n",
    "# M√£ h√≥a t·ª´ng c·ªôt\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])  # Thay ƒë·ªïi tr·ª±c ti·∫øp gi√° tr·ªã trong c·ªôt\n",
    "    decoders[col] = dict(zip(le.classes_, le.transform(le.classes_)))  # L∆∞u mapping cho b√°o c√°o\n",
    "\n",
    "# Xu·∫•t b√°o c√°o m√£ h√≥a\n",
    "encoding_report = \"\"\"B√°o c√°o M√£ h√≥a Categorical Data\\n\\n\"\"\"\n",
    "for col, mapping in decoders.items():\n",
    "    encoding_report += f\"C·ªôt: {col}\\n\"\n",
    "    for key, value in mapping.items():\n",
    "        encoding_report += f\"  {key}: {value}\\n\"\n",
    "    encoding_report += \"\\n\"\n",
    "\n",
    "# L∆∞u dataset ƒë√£ m√£ h√≥a\n",
    "df.to_csv('new_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f43555e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd \n",
    "import networkx as nx \n",
    "from itertools import combinations\n",
    "from torch_geometric.data import Data \n",
    "from model import GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cada523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªçc dataset\n",
    "file_path = r'D:\\NƒÉm 3 - HK2\\M·∫°ng x√£ h·ªôi\\traffic-accident-analysis\\data\\new_data_cleaned.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bbc35ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ t·∫°o ƒë·ªì th·ªã v·ªõi 100 n√∫t v√† 4156 c·∫°nh.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "G = nx.Graph()\n",
    "for index, row in df.iterrows():\n",
    "    G.add_node(index, **row.to_dict())\n",
    "\n",
    "def is_similar(accident1, accident2):\n",
    "    return (\n",
    "        abs(accident1['crash_hour'] - accident2['crash_hour']) <= 1 or\n",
    "        accident1['crash_month'] == accident2['crash_month'] or\n",
    "        accident1['crash_day_of_week'] == accident2['crash_day_of_week'] or\n",
    "        accident1['trafficway_type'] == accident2['trafficway_type'] or\n",
    "        accident1['first_crash_type'] == accident2['first_crash_type'] or\n",
    "        accident1['injuries_no_indication'] == accident2['injuries_no_indication']\n",
    "    )\n",
    "\n",
    "for u, v in combinations(G.nodes(data=True), 2):\n",
    "    if is_similar(u[1], v[1]):\n",
    "        G.add_edge(u[0], v[0])\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ t·∫°o ƒë·ªì th·ªã v·ªõi {G.number_of_nodes()} n√∫t v√† {G.number_of_edges()} c·∫°nh.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "812aec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def networkx_to_pyg_inference(G):\n",
    "    feature_attrs = list(next(iter(G.nodes(data=True)))[1].keys())\n",
    "\n",
    "    features = [\n",
    "        [float(data[attr]) for attr in feature_attrs]\n",
    "        for _, data in G.nodes(data=True)\n",
    "    ]\n",
    "\n",
    "    x = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "    node_mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    edge_index = torch.tensor(\n",
    "        [[node_mapping[u], node_mapping[v]] for u, v in G.edges()],\n",
    "        dtype=torch.long\n",
    "    ).t().contiguous()\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "data_new = networkx_to_pyg_inference(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42bddece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT(\n",
       "  (gat1): GATConv(22, 16, heads=8)\n",
       "  (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (gat2): GATConv(128, 16, heads=4)\n",
       "  (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (gat3): GATConv(64, 3, heads=1)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GAT(in_features=22, hidden_dim=16, out_features=3, heads=8).to(device)\n",
    "model.load_state_dict(torch.load('gat_model.pth', map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06217ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(data_new.x.to(device), data_new.edge_index.to(device))\n",
    "    pred = out.argmax(dim=1)\n",
    "\n",
    "df['predicted_damage'] = pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c58a827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    predicted_damage predicted_damage_label\n",
      "0                  0           $500 OR LESS\n",
      "1                  0           $500 OR LESS\n",
      "2                  0           $500 OR LESS\n",
      "3                  0           $500 OR LESS\n",
      "4                  0           $500 OR LESS\n",
      "..               ...                    ...\n",
      "95                 1          $501 - $1,500\n",
      "96                 1          $501 - $1,500\n",
      "97                 1          $501 - $1,500\n",
      "98                 1          $501 - $1,500\n",
      "99                 1          $501 - $1,500\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Mapping s·ªë v·ªÅ chu·ªói nh∆∞ trong c·ªôt damage g·ªëc\n",
    "damage_mapping = {\n",
    "    0: \"$500 OR LESS\",\n",
    "    1: \"$501 - $1,500\",\n",
    "    2: \"OVER $1,500\"\n",
    "}\n",
    "\n",
    "# T·∫°o c·ªôt m·ªõi v·ªõi nh√£n d·∫°ng chu·ªói\n",
    "df['predicted_damage_label'] = df['predicted_damage'].map(damage_mapping)\n",
    "\n",
    "# Hi·ªÉn th·ªã 5 d√≤ng ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra\n",
    "print(df[['predicted_damage', 'predicted_damage_label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe7eb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def analyze_high_damage_nodes(G, label_attr='predicted_damage', high_level=1, top_k=5):\n",
    "    # B∆∞·ªõc 1: L·ªçc c√°c node c√≥ damage cao\n",
    "    high_damage_nodes = [n for n, data in G.nodes(data=True) if data.get(label_attr) == high_level]\n",
    "\n",
    "    # B∆∞·ªõc 2: Th·ªëng k√™ c√°c feature xu·∫•t hi·ªán trong nh√≥m n√†y\n",
    "    feature_counters = defaultdict(Counter)\n",
    "\n",
    "    for n in high_damage_nodes:\n",
    "        node_data = G.nodes[n]\n",
    "        for attr, value in node_data.items():\n",
    "            if attr != label_attr:\n",
    "                feature_counters[attr][value] += 1\n",
    "\n",
    "    # B∆∞·ªõc 3: In ra top K gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t cho t·ª´ng feature\n",
    "    print(f\"\\nüìä Top nguy√™n nh√¢n th∆∞·ªùng th·∫•y khi `{label_attr} = {high_level}`:\\n\")\n",
    "    for attr, counter in feature_counters.items():\n",
    "        print(f\"- {attr}:\")\n",
    "        for val, freq in counter.most_common(top_k):\n",
    "            print(f\"   ‚Ä¢ {val}: {freq} l·∫ßn\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9438315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Top nguy√™n nh√¢n th∆∞·ªùng th·∫•y khi `predicted_damage = 1`:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gi·∫£ s·ª≠ b·∫°n ƒë√£ c√≥ ƒë·ªì th·ªã G\n",
    "analyze_high_damage_nodes(G, label_attr='predicted_damage', high_level=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
